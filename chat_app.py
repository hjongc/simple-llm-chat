import streamlit as st
import requests
import logging
import json
import uuid
from datetime import datetime
from llm_client import chat_with_context, chat_with_context_stream

# ì„œë²„ ì„¤ì •
SERVER_CHAT_API = "http://localhost:9393/v1/chat/completions"
logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')

# ë§ˆí¬ë‹¤ìš´ ì´ìŠ¤ì¼€ì´í”„ í•¨ìˆ˜
def escape_markdown(text):
    """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ë§ˆí¬ë‹¤ìš´ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬"""
    if not text:
        return text
    
    # ë§ˆí¬ë‹¤ìš´ íŠ¹ìˆ˜ë¬¸ìë“¤ì„ ì´ìŠ¤ì¼€ì´í”„
    markdown_chars = ['*', '_', '`', '#', '[', ']', '(', ')', '>', '!', '|', '\\', '~', '^']
    for char in markdown_chars:
        text = text.replace(char, f'\\{char}')
    
    # ìˆ˜í•™ ìˆ˜ì‹ ë°©ì§€ ($ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¶€ë¶„)
    text = text.replace('$', '\\$')
    
    return text

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
defaults = {
    "session_id": str(uuid.uuid4()),
    "chat_history": [],
    "recent_inputs": [],
    "conversation_context": [],  # ëŒ€í™” ë§¥ë½ ì €ì¥ìš©
    "show_welcome": True,
    "last_response": ""
}
for key, val in defaults.items():
    st.session_state.setdefault(key, val)

# --- ì‹ ê·œ: ì±„íŒ… ì œì¶œ ì²˜ë¦¬ í•¨ìˆ˜ ---
def handle_chat_submission(prompt):
    """ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ LLM ì‘ë‹µì„ ì²˜ë¦¬í•˜ê³  í‘œì‹œí•˜ëŠ” í†µí•© í•¨ìˆ˜"""
    st.session_state.show_welcome = False
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
    with st.chat_message("user"):
        st.text(prompt)
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    st.session_state.recent_inputs.append(prompt)
    if len(st.session_state.recent_inputs) > 10:
        st.session_state.recent_inputs = st.session_state.recent_inputs[-10:]

    # CoT ì²˜ë¦¬
    processed_prompt = prompt
    if st.session_state.get("use_cot"):
        processed_prompt += "\n\n**Chain of Thought ìš”ì²­:** ìƒê°í•˜ëŠ” ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„œìˆ í•œ ë’¤, ë§ˆì§€ë§‰ì— í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”."

    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ
    with st.chat_message("assistant"):
        full_response = ""
        try:
            if st.session_state.get("use_streaming", True):
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                response_placeholder = st.empty()
                for chunk in chat_with_context_stream(
                    message=processed_prompt,
                    conversation_history=st.session_state.conversation_context,
                    server_url=SERVER_CHAT_API,
                    model_name=st.session_state.get("model_name", "gpt-4o"),
                    temperature=st.session_state.get("temperature", 0.7),
                    max_tokens=st.session_state.get("max_tokens", 1024)
                ):
                    if chunk:
                        full_response += chunk
                        response_placeholder.markdown(full_response + "â–Œ")
                response_placeholder.markdown(full_response)
            else:
                # ì¼ë°˜ ì‘ë‹µ
                with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘..."):
                    full_response = chat_with_context(
                        message=processed_prompt,
                        conversation_history=st.session_state.conversation_context,
                        server_url=SERVER_CHAT_API,
                        model_name=st.session_state.get("model_name", "gpt-4o"),
                        temperature=st.session_state.get("temperature", 0.7),
                        max_tokens=st.session_state.get("max_tokens", 1024)
                    )
                    st.markdown(full_response)
        except Exception as e:
            full_response = f"[ì˜¤ë¥˜] ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.markdown(full_response)

    # ì‘ë‹µ ë° ëŒ€í™” ë§¥ë½ ì €ì¥
    st.session_state.chat_history.append({"role": "assistant", "content": full_response})
    st.session_state.last_response = full_response
    st.session_state.conversation_context.append({"role": "user", "content": prompt})
    st.session_state.conversation_context.append({"role": "assistant", "content": full_response})
    if len(st.session_state.conversation_context) > 20:
        st.session_state.conversation_context = st.session_state.conversation_context[-20:]

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(
    page_title="MI Project Agent", 
    page_icon="ğŸ¤–", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# ê¸°ë³¸ CSS ìŠ¤íƒ€ì¼
st.markdown("""
    <style>
.welcome-container {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    padding: 2rem;
    border-radius: 1rem;
    text-align: center;
    color: white;
    margin-bottom: 2rem;
}
.feature-card {
    background-color: #f8f9fa;
    border: 1px solid #e9ecef;
    padding: 1rem;
    border-radius: 0.5rem;
    margin: 0.5rem 0;
}
.stButton > button {
    width: 100%;
    border-radius: 4px;
    font-size: 13px;
    padding: 4px 8px;
    height: 32px;
    }
    </style>

<script>
function copyToClipboard(text) {
    navigator.clipboard.writeText(text).then(function() {
        // ì„±ê³µ ì‹œ ì•Œë¦¼ í‘œì‹œ
        const toast = document.createElement('div');
        toast.textContent = 'âœ… ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!';
        toast.style.cssText = `
            position: fixed;
            top: 20px;
            right: 20px;
            background: #28a745;
            color: white;
            padding: 10px 15px;
            border-radius: 5px;
            z-index: 9999;
            font-size: 14px;
        `;
        document.body.appendChild(toast);
        setTimeout(() => {
            document.body.removeChild(toast);
        }, 2000);
    }, function(err) {
        console.error('ë³µì‚¬ ì‹¤íŒ¨: ', err);
        alert('ë³µì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì €ê°€ í´ë¦½ë³´ë“œ APIë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.');
    });
}
</script>
""", unsafe_allow_html=True)

# í—¤ë”
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    st.markdown("<h1 style='text-align: center; margin-bottom: 1rem;'>ğŸ¤– MI Project Agent</h1>", unsafe_allow_html=True)

# í™˜ì˜ í˜ì´ì§€
if st.session_state.show_welcome and not st.session_state.chat_history:
    st.markdown("""
    <div class="welcome-container">
        <h2>ğŸ‘‹ ì•ˆë…•í•˜ì„¸ìš”! MI Project Agentì…ë‹ˆë‹¤</h2>
        <p>MI í”„ë¡œì íŠ¸ì™€ IT ì¸í”„ë¼ ê´€ë ¨ ì „ë¬¸ ìƒë‹´ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤</p>
        <br>
        <div style="display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap;">
            <div style="text-align: center;">
                <div style="font-size: 2rem;">ğŸ—„ï¸</div>
                <strong>ë°ì´í„°ë² ì´ìŠ¤</strong><br>
                <small>Oracle, SQL Server, MySQL ê´€ë¦¬</small>
            </div>
            <div style="text-align: center;">
                <div style="font-size: 2rem;">âš™ï¸</div>
                <strong>Control-M</strong><br>
                <small>ì‘ì—… ìŠ¤ì¼€ì¤„ë§ ë° ìë™í™”</small>
            </div>
            <div style="text-align: center;">
                <div style="font-size: 2rem;">ğŸ“Š</div>
                <strong>MSTR ë¶„ì„</strong><br>
                <small>ë¦¬í¬íŠ¸ ì„¤ê³„ ë° ìµœì í™”</small>
            </div>
            <div style="text-align: center;">
                <div style="font-size: 2rem;">ğŸ”§</div>
                <strong>ì‹œìŠ¤í…œ ê´€ë¦¬</strong><br>
                <small>ì„œë²„ ë° ë„¤íŠ¸ì›Œí¬ ê´€ë¦¬</small>
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("### ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì—¬ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”")
    st.markdown("**ì˜ˆì‹œ ì§ˆë¬¸:**")
    st.markdown("- MSTR ë¦¬í¬íŠ¸ ì„¤ê³„ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”")
    st.markdown("- Oracle ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™” ë°©ë²•")
    st.markdown("- Control-M ì‘ì—… ìŠ¤ì¼€ì¤„ë§ ì„¤ì • ë°©ë²•")
    st.markdown("- SQL ì¿¼ë¦¬ íŠœë‹ íŒ")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    st.session_state.model_name = st.selectbox(
        "ğŸ§  ëª¨ë¸", 
        [
            "gpt-4o", 
            "gpt-4o-mini",
            "gpt-4.1"
        ], 
        index=0
    )
    
    with st.expander("ğŸ”§ ê³ ê¸‰ ì„¤ì •", expanded=False):
        st.session_state.temperature = st.slider("ğŸ”¥ Temperature", 0.0, 1.5, 0.7, 0.1, help="ì‘ë‹µì˜ ì°½ì˜ì„± ì¡°ì ˆ")
        st.session_state.max_tokens = st.slider("ğŸ§± Max Tokens", 50, 4096, 1024, step=10, help="ìµœëŒ€ ì‘ë‹µ ê¸¸ì´")
        st.session_state.use_cot = st.checkbox("ğŸ§  Chain of Thought", value=False, help="ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì • ìš”ì²­")
        st.session_state.use_streaming = st.checkbox("ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ", value=True, help="ì‹¤ì‹œê°„ ì‘ë‹µ í‘œì‹œ")
    
    st.divider()
    
    # ëŒ€í™” ê´€ë¦¬
    st.subheader("ğŸ’¬ ëŒ€í™” ê´€ë¦¬")
    if st.button("ğŸ§¹ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.chat_history = []
        st.session_state.conversation_context = []
        st.session_state.show_welcome = True
        st.success("ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.rerun()

# ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
if st.session_state.chat_history:
    st.session_state.show_welcome = False
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for i, message in enumerate(st.session_state.chat_history):
        with st.chat_message(message["role"]):
            if message["role"] == "user":
                # ì‚¬ìš©ì ì…ë ¥ì€ ë§ˆí¬ë‹¤ìš´ ì—†ì´ ê·¸ëŒ€ë¡œ í‘œì‹œ
                st.text(message["content"])
            else:
                # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µì€ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
                st.markdown(message["content"])
                

# ì±„íŒ… ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    handle_chat_submission(prompt)

# ìµœê·¼ ì§ˆë¬¸ (í•˜ë‹¨ì— í‘œì‹œ)
if st.session_state.recent_inputs and not st.session_state.show_welcome:
    with st.expander("ğŸ“Œ ìµœê·¼ ì§ˆë¬¸", expanded=False):
        for i, recent_q in enumerate(reversed(st.session_state.recent_inputs[-5:])):
            if st.button(f"ğŸ”„ {recent_q[:50]}{'...' if len(recent_q) > 50 else ''}", 
                        key=f"recent_{i}", use_container_width=True):
                # í†µí•© í•¨ìˆ˜ í˜¸ì¶œ (st.rerun() ì œê±°)
                handle_chat_submission(recent_q)
                st.rerun() # UI ì¦‰ì‹œ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ rerun ì¶”ê°€
