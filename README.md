# Simple LLM Chat

> ë‚´ë¶€ LLM ì„œë²„ë¥¼ í™œìš©í•˜ëŠ” ê°„ë‹¨í•œ ì±„íŒ… ì•± (í†µì‹ ì‚¬ íì‡„ë§ í™˜ê²½ì—ì„œ ì‹¤ì œ ì‚¬ìš© ì¤‘)
> Simple chat app using internal LLM server (Actually used in telecom isolated network)

<div align="center">

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)

**[í•œêµ­ì–´](#-ì†Œê°œ)** | **[English](#english-version)**

</div>

## ğŸ“ ì†Œê°œ

**Simple LLM Chat**ì€ ë‚´ë¶€ LLM API ì„œë²„ë¥¼ í™œìš©í•œ ì±„íŒ… ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ê²½ëŸ‰ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.
í†µì‹ ì‚¬ íì‡„ë§ í™˜ê²½ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•  ëª©ì ìœ¼ë¡œ ê°œë°œë˜ì—ˆìœ¼ë©°, êµ¬ì„±ì›ë“¤ì´ ë§¤ìš° í¸í•˜ê²Œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### ğŸ¯ ì‚¬ìš© ì‚¬ë¡€

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤:

- **ë‚´ë¶€ LLM ì„œë²„ í™œìš©**: ê¸°ì—… ë‚´ë¶€ì— LLM API í”„ë¡ì‹œ ì„œë²„ê°€ êµ¬ì¶•ë˜ì–´ ìˆëŠ” ê²½ìš°
- **OpenAI í˜¸í™˜ API**: ë‚´ë¶€ ì„œë²„ê°€ OpenAI Chat Completion API í˜•ì‹ì„ ì§€ì›í•˜ëŠ” ê²½ìš°

**ì‹¤ì œ êµ¬ì„± ì˜ˆì‹œ:**
```
[ì‚¬ìš©ì PC] â†’ [Simple LLM Chat] â†’ [ë‚´ë¶€ LLM í”„ë¡ì‹œ ì„œë²„] â†’ [LLM ëª¨ë¸]
                  (ë³¸ ì•±)              (ë‚´ë¶€ ì¸í”„ë¼)          (GPT-4 ë“±)
```

### ì£¼ìš” íŠ¹ì§•

- ğŸš€ **ê°„ë‹¨í•œ êµ¬ì„±**: FastAPI í”„ë¡ì‹œ + Streamlit UIë¡œ êµ¬ì„±ëœ ì‹¬í”Œí•œ ì•„í‚¤í…ì²˜
- ğŸ”„ **OpenAI í˜¸í™˜ API**: OpenAI Chat Completion APIì™€ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤
- ğŸ’¬ **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì§€ì›í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹µë³€ í™•ì¸ ê°€ëŠ¥
- ğŸ¨ **ì§ê´€ì ì¸ UI**: Streamlit ê¸°ë°˜ì˜ ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
- ğŸ“Š **ëŒ€í™” ë§¥ë½ ìœ ì§€**: ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ì—¬ ì—°ì†ì ì¸ ëŒ€í™” ê°€ëŠ¥

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### ì‹œìŠ¤í…œ êµ¬ì„±ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬                          â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  Streamlit UI   â”‚  (Port 9191)                      â”‚
â”‚  â”‚   (Web Chat)    â”‚  - ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - ì±„íŒ… ê¸°ëŠ¥ ì œê³µ                  â”‚
â”‚           â”‚                                             â”‚
â”‚           â†“ HTTP Request                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  FastAPI Server â”‚  (Port 9393)                      â”‚
â”‚  â”‚  (API Proxy)    â”‚  - OpenAI í˜¸í™˜ API                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - ìŠ¤íŠ¸ë¦¬ë° ì§€ì›                   â”‚
â”‚           â”‚                                             â”‚
â”‚           â†“ API Call (ë‚´ë¶€ë§)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚   ë‚´ë¶€ LLM í”„ë¡ì‹œ ì„œë²„         â”‚                      â”‚
â”‚  â”‚  (ì˜ˆ: Azure OpenAI Service) â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë™ì‘ ë°©ì‹

1. **ì‚¬ìš©ì â†’ Streamlit UI**: ì§ì›ì´ ì›¹ ë¸Œë¼ìš°ì €ë¡œ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ì ‘ì†
2. **Streamlit â†’ FastAPI**: UIê°€ ë¡œì»¬ FastAPI ì„œë²„(9393 í¬íŠ¸)ë¡œ ìš”ì²­ ì „ì†¡
3. **FastAPI â†’ ë‚´ë¶€ LLM ì„œë²„**: FastAPIê°€ ê¸°ì—… ë‚´ë¶€ LLM API ì„œë²„ë¡œ í”„ë¡ì‹œ
4. **LLM ì‘ë‹µ â†’ ì‚¬ìš©ì**: ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µ ì „ë‹¬

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

ì´ ì•±ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒì´ í•„ìš”í•©ë‹ˆë‹¤:

- âœ… **ë‚´ë¶€ LLM API ì„œë²„**: OpenAI Chat Completion APIì™€ í˜¸í™˜ë˜ëŠ” ë‚´ë¶€ ì„œë²„ í•„ìš”
- âœ… **API ì¸ì¦ í‚¤**: ë‚´ë¶€ LLM ì„œë²„ ì ‘ê·¼ì„ ìœ„í•œ ì¸ì¦ í‚¤
- âœ… **ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼**: íì‡„ë§ ë‚´ì—ì„œ ë‚´ë¶€ LLM ì„œë²„ë¡œì˜ HTTP/HTTPS í†µì‹  ê°€ëŠ¥í•´ì•¼ í•¨

**ì§€ì›í•˜ëŠ” LLM API ì„œë²„ ì˜ˆì‹œ:**
- Azure OpenAI Service (ê¸°ì—… ì „ìš©)
- AWS Bedrock (VPC ë‚´ë¶€)
- ìì²´ êµ¬ì¶• LLM Gateway
- OpenAI API í”„ë¡ì‹œ ì„œë²„

## ğŸ“¦ ì„¤ì¹˜

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

- Python 3.8 ì´ìƒ
- pip

### ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

## âš™ï¸ ì„¤ì •

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env.example` íŒŒì¼ì„ ë³µì‚¬í•˜ì—¬ `.env` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

```bash
cp .env.example .env
```

`.env` íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤:

```env
# LLM API ì¸ì¦ í‚¤ (í•„ìˆ˜)
API_KEY=your_api_key_here

# LLM API ì—”ë“œí¬ì¸íŠ¸ URL (í•„ìˆ˜)
LLM_API_BASE_URL=https://your-internal-llm-api.com/v1/chat/completions

# ì„œë²„ í¬íŠ¸ ì„¤ì • (ì„ íƒì‚¬í•­)
FASTAPI_PORT=9393
STREAMLIT_PORT=9191
```

**ì£¼ìš” í™˜ê²½ ë³€ìˆ˜:**
- `API_KEY`: LLM API ì¸ì¦ í‚¤ (í•„ìˆ˜)
- `LLM_API_BASE_URL`: ë‚´ë¶€ LLM API ì„œë²„ ì£¼ì†Œ (í•„ìˆ˜)
- `FASTAPI_PORT`: FastAPI ì„œë²„ í¬íŠ¸ (ê¸°ë³¸ê°’: 9393)
- `STREAMLIT_PORT`: Streamlit ì•± í¬íŠ¸ (ê¸°ë³¸ê°’: 9191)
- `API_TIMEOUT`: API íƒ€ì„ì•„ì›ƒ ì´ˆ (ê¸°ë³¸ê°’: 30)
- `LOG_LEVEL`: ë¡œê·¸ ë ˆë²¨ (ê¸°ë³¸ê°’: INFO)

ì „ì²´ í™˜ê²½ ë³€ìˆ˜ ëª©ë¡ì€ [.env.example](.env.example) íŒŒì¼ì„ ì°¸ê³ í•˜ì„¸ìš”.

## ğŸš€ ì‹¤í–‰

### 1. FastAPI í”„ë¡ì‹œ ì„œë²„ ì‹œì‘

```bash
bash scripts/start_server.sh
```

ì„œë²„ëŠ” `http://localhost:9393`ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.

### 2. Streamlit ì›¹ ì•± ì‹œì‘

```bash
bash scripts/start_app.sh
```

ì›¹ ì•±ì€ `http://localhost:9191`ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.

### 3. ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†

```
http://localhost:9191
```

## ğŸ›‘ ì¤‘ì§€

### FastAPI ì„œë²„ ì¤‘ì§€

```bash
bash scripts/stop_server.sh
```

### Streamlit ì•± ì¤‘ì§€

```bash
bash scripts/stop_app.sh
```

## ğŸ“– ì‚¬ìš©ë²•

### ê¸°ë³¸ ì±„íŒ…

1. ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:9191` ì ‘ì†
2. í™”ë©´ í•˜ë‹¨ì˜ ì±„íŒ… ì…ë ¥ì°½ì— ì§ˆë¬¸ ì…ë ¥
3. ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë˜ëŠ” ë‹µë³€ í™•ì¸

### ì„¤ì • ë³€ê²½

ì‚¬ì´ë“œë°”ì—ì„œ ë‹¤ìŒ ì„¤ì •ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ëª¨ë¸ ì„ íƒ**: gpt-4o, gpt-4o-mini ë“±
- **Temperature**: ì‘ë‹µì˜ ì°½ì˜ì„± ì¡°ì ˆ (0.0 ~ 1.5)
- **Max Tokens**: ìµœëŒ€ ì‘ë‹µ ê¸¸ì´ (50 ~ 4096)
- **Chain of Thought**: ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì • ìš”ì²­
- **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ**: ì‹¤ì‹œê°„ ì‘ë‹µ í‘œì‹œ On/Off

### ëŒ€í™” ì´ˆê¸°í™”

ì‚¬ì´ë“œë°”ì˜ "ğŸ§¹ ëŒ€í™” ì´ˆê¸°í™”" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëŒ€í™” ë‚´ì—­ì„ ì´ˆê¸°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

ëª¨ë“  ì„¤ì •ì€ `.env` íŒŒì¼ì„ í†µí•´ ê´€ë¦¬ë©ë‹ˆë‹¤.

### ì£¼ìš” ì„¤ì • í•­ëª©

**API ì„¤ì •:**
```env
API_TIMEOUT=30           # API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
API_MAX_RETRIES=3        # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
API_RETRY_DELAY=1        # ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
```

**í¬íŠ¸ ì„¤ì •:**
```env
FASTAPI_PORT=9393        # FastAPI ì„œë²„ í¬íŠ¸
STREAMLIT_PORT=9191      # Streamlit ì•± í¬íŠ¸
```

**ë¡œê¹… ì„¤ì •:**
```env
LOG_LEVEL=INFO           # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_DIR=logs             # ë¡œê·¸ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
```

**CORS ì„¤ì •:**
```env
CORS_ORIGINS=*           # í—ˆìš©í•  ì¶œì²˜ (ì½¤ë§ˆë¡œ êµ¬ë¶„)
# í”„ë¡œë•ì…˜ ì˜ˆì‹œ: CORS_ORIGINS=https://your-domain.com,https://app.your-domain.com
```

ì„¤ì • ë³€ê²½ í›„ ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ë©´ ì¦‰ì‹œ ì ìš©ë©ë‹ˆë‹¤.

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
simple-llm-chat/
â”œâ”€â”€ README.md              # í”„ë¡œì íŠ¸ ë¬¸ì„œ
â”œâ”€â”€ requirements.txt       # Python íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
â”œâ”€â”€ .env                   # í™˜ê²½ ë³€ìˆ˜ (gitì—ì„œ ì œì™¸)
â”œâ”€â”€ .env.example          # í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿
â”œâ”€â”€ .gitignore            # Git ì œì™¸ íŒŒì¼
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py       # íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
â”‚   â”œâ”€â”€ config.py         # ì„¤ì • ê´€ë¦¬
â”‚   â”œâ”€â”€ server.py         # FastAPI í”„ë¡ì‹œ ì„œë²„
â”‚   â”œâ”€â”€ client.py         # LLM API í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ app.py            # Streamlit ì›¹ ì±„íŒ… UI
â”‚   â””â”€â”€ utils/            # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start_server.sh   # FastAPI ì„œë²„ ì‹œì‘
â”‚   â”œâ”€â”€ start_app.sh      # Streamlit ì•± ì‹œì‘
â”‚   â”œâ”€â”€ stop_server.sh    # FastAPI ì„œë²„ ì¤‘ì§€
â”‚   â””â”€â”€ stop_app.sh       # Streamlit ì•± ì¤‘ì§€
â””â”€â”€ logs/                 # ë¡œê·¸ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ìë™ ìƒì„±)
```

## ğŸ” API ì—”ë“œí¬ì¸íŠ¸

### POST /v1/chat/completions

OpenAI Chat Completion APIì™€ í˜¸í™˜ë˜ëŠ” ì±„íŒ… ì™„ì„± ì—”ë“œí¬ì¸íŠ¸

**ìš”ì²­ ì˜ˆì‹œ:**

```json
{
  "model": "gpt-4o",
  "messages": [
    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}
  ],
  "stream": false,
  "temperature": 0.7,
  "max_tokens": 1024
}
```

**ì‘ë‹µ ì˜ˆì‹œ:**

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "gpt-4o",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 15,
    "total_tokens": 25
  }
}
```

### GET /health

ì„œë²„ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸

### GET /stats

ì„œë²„ í†µê³„ ì •ë³´ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì„œë²„ê°€ ì‹œì‘ë˜ì§€ ì•ŠëŠ” ê²½ìš°

1. í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸:
   ```bash
   lsof -i :9393  # FastAPI ì„œë²„
   lsof -i :9191  # Streamlit ì•±
   ```

2. ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ:
   ```bash
   bash scripts/stop_server.sh
   bash scripts/stop_app.sh
   ```

### API í‚¤ ì˜¤ë¥˜

`.env` íŒŒì¼ì— ì˜¬ë°”ë¥¸ `API_KEY`ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

### ë¡œê·¸ í™•ì¸

- **FastAPI ì„œë²„**: `logs/uvicorn_YYYY-MM-DD.log`
- **Streamlit ì•±**: `logs/chat_app_YYYY-MM-DD.log`

## ğŸ¤ ê¸°ì—¬

ì´ìŠˆ ë° Pull RequestëŠ” ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤!

## ğŸ“§ ë¬¸ì˜

í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì€ ì´ìŠˆë¡œ ë“±ë¡í•´ì£¼ì„¸ìš”.

---

<div align="center">
Made with â¤ï¸ for Internal LLM Server Environments
</div>

---
---

# English Version

> Simple chat app using internal LLM server

## ğŸ“ Introduction

**Simple LLM Chat** is a lightweight chat application using internal LLM API server.
Originally developed for use in a telecom company's isolated network, it has been successfully adopted by team members for daily use.

### ğŸ¯ Use Case

This project is designed for environments with:

- **Internal LLM Server**: Organizations with in-house LLM API infrastructure
- **OpenAI-Compatible API**: Internal servers supporting OpenAI Chat Completion API format

**Real-world Architecture:**
```
[User PC] â†’ [Simple LLM Chat] â†’ [Internal LLM Proxy] â†’ [LLM Model]
              (This App)          (Internal Infra)      (GPT-4, etc.)
```

### Key Features

- ğŸš€ **Simple Architecture**: FastAPI proxy + Streamlit UI
- ğŸ”„ **OpenAI Compatible**: Compatible with OpenAI Chat Completion API
- ğŸ’¬ **Real-time Streaming**: Supports streaming responses for instant feedback
- ğŸ¨ **Intuitive UI**: Easy-to-use Streamlit-based chat interface
- ğŸ“Š **Context Awareness**: Maintains conversation history for continuous dialogue

## ğŸ—ï¸ Architecture

### System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Internal Network                      â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  Streamlit UI   â”‚  (Port 9191)                      â”‚
â”‚  â”‚   (Web Chat)    â”‚  - User Interface                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Chat Features                  â”‚
â”‚           â”‚                                             â”‚
â”‚           â†“ HTTP Request                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  FastAPI Server â”‚  (Port 9393)                      â”‚
â”‚  â”‚  (API Proxy)    â”‚  - OpenAI Compatible              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Streaming Support              â”‚
â”‚           â”‚                                             â”‚
â”‚           â†“ API Call (Internal Network)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚   Internal LLM Proxy Server  â”‚                      â”‚
â”‚  â”‚  (e.g., Azure OpenAI)        â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. **User â†’ Streamlit UI**: Employee accesses chat interface via web browser
2. **Streamlit â†’ FastAPI**: UI sends requests to local FastAPI server (port 9393)
3. **FastAPI â†’ Internal LLM**: FastAPI proxies to internal LLM API server
4. **LLM Response â†’ User**: Streaming response delivered in real-time

### Requirements

To use this app, you need:

- âœ… **Internal LLM API Server**: OpenAI Chat Completion API compatible server
- âœ… **API Authentication Key**: Credentials for internal LLM server access
- âœ… **Network Access**: HTTP/HTTPS connectivity to internal LLM server within airgap

**Supported LLM API Servers:**
- Azure OpenAI Service (Enterprise)
- AWS Bedrock (VPC Internal)
- Self-hosted LLM Gateway
- OpenAI API Proxy Server

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8+
- pip

### Install Dependencies

```bash
pip install -r requirements.txt
```

## âš™ï¸ Configuration

### 1. Environment Variables

Copy `.env.example` to create `.env`:

```bash
cp .env.example .env
```

Edit `.env` file with your configuration:

```env
# LLM API Authentication Key (Required)
API_KEY=your_api_key_here

# LLM API Endpoint URL (Required)
LLM_API_BASE_URL=https://your-internal-llm-api.com/v1/chat/completions

# Server Port Configuration (Optional)
FASTAPI_PORT=9393
STREAMLIT_PORT=9191
```

**Key Environment Variables:**
- `API_KEY`: LLM API authentication key (Required)
- `LLM_API_BASE_URL`: Internal LLM API server address (Required)
- `FASTAPI_PORT`: FastAPI server port (Default: 9393)
- `STREAMLIT_PORT`: Streamlit app port (Default: 9191)
- `API_TIMEOUT`: API timeout in seconds (Default: 30)
- `LOG_LEVEL`: Log level (Default: INFO)

See [.env.example](.env.example) for complete list of environment variables.

## ğŸš€ Usage

### 1. Start FastAPI Proxy Server

```bash
bash scripts/start_server.sh
```

Server runs at `http://localhost:9393`

### 2. Start Streamlit Web App

```bash
bash scripts/start_app.sh
```

Web app runs at `http://localhost:9191`

### 3. Access in Browser

```
http://localhost:9191
```

## ğŸ›‘ Stop Services

### Stop FastAPI Server

```bash
bash scripts/stop_server.sh
```

### Stop Streamlit App

```bash
bash scripts/stop_app.sh
```

## ğŸ“– User Guide

### Basic Chat

1. Access `http://localhost:9191` in web browser
2. Enter your question in the chat input at the bottom
3. View streaming response in real-time

### Settings

Adjust settings in the sidebar:

- **Model Selection**: gpt-4o, gpt-4o-mini, etc.
- **Temperature**: Control response creativity (0.0 ~ 1.5)
- **Max Tokens**: Maximum response length (50 ~ 4096)
- **Chain of Thought**: Request step-by-step reasoning
- **Streaming Response**: Toggle real-time display On/Off

### Clear Conversation

Click "ğŸ§¹ Clear Conversation" button in sidebar to reset chat history.

## ğŸ”§ Advanced Configuration

All settings are managed via `.env` file.

### Key Configuration Items

**API Settings:**
```env
API_TIMEOUT=30           # API call timeout (seconds)
API_MAX_RETRIES=3        # Maximum retry attempts
API_RETRY_DELAY=1        # Retry delay (seconds)
```

**Port Settings:**
```env
FASTAPI_PORT=9393        # FastAPI server port
STREAMLIT_PORT=9191      # Streamlit app port
```

**Logging Settings:**
```env
LOG_LEVEL=INFO           # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_DIR=logs             # Log file directory
```

**CORS Settings:**
```env
CORS_ORIGINS=*           # Allowed origins (comma-separated)
# Production example: CORS_ORIGINS=https://your-domain.com,https://app.your-domain.com
```

Changes take effect after server restart.

## ğŸ“‚ Project Structure

```
isolated-chat/
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env                   # Environment variables (gitignored)
â”œâ”€â”€ .env.example          # Environment variable template
â”œâ”€â”€ .gitignore            # Git ignore rules
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py       # Package initialization
â”‚   â”œâ”€â”€ config.py         # Configuration management
â”‚   â”œâ”€â”€ server.py         # FastAPI proxy server
â”‚   â”œâ”€â”€ client.py         # LLM API client
â”‚   â”œâ”€â”€ app.py            # Streamlit web UI
â”‚   â””â”€â”€ utils/            # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start_server.sh   # Start FastAPI server
â”‚   â”œâ”€â”€ start_app.sh      # Start Streamlit app
â”‚   â”œâ”€â”€ stop_server.sh    # Stop FastAPI server
â”‚   â””â”€â”€ stop_app.sh       # Stop Streamlit app
â””â”€â”€ logs/                 # Log directory (auto-created)
```

## ğŸ” API Endpoints

### POST /v1/chat/completions

OpenAI Chat Completion API compatible endpoint

**Request Example:**

```json
{
  "model": "gpt-4o",
  "messages": [
    {"role": "user", "content": "Hello"}
  ],
  "stream": false,
  "temperature": 0.7,
  "max_tokens": 1024
}
```

**Response Example:**

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "gpt-4o",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 15,
    "total_tokens": 25
  }
}
```

### GET /health

Server health check endpoint

### GET /stats

Server statistics endpoint

## ğŸ› ï¸ Troubleshooting

### Server Won't Start

1. Check if port is already in use:
   ```bash
   lsof -i :9393  # FastAPI server
   lsof -i :9191  # Streamlit app
   ```

2. Stop existing processes:
   ```bash
   bash scripts/stop_server.sh
   bash scripts/stop_app.sh
   ```

### API Key Error

Verify correct `API_KEY` is set in `.env` file.

### Check Logs

- **FastAPI Server**: `logs/uvicorn_YYYY-MM-DD.log`
- **Streamlit App**: `logs/chat_app_YYYY-MM-DD.log`

## ğŸ¤ Contributing

Issues and Pull Requests are welcome!

## ğŸ“§ Contact

Please submit issues for any questions or feedback.

---

<div align="center">
Made with â¤ï¸ for Internal LLM Server Environments
</div>
